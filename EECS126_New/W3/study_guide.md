# Modes of Convergence

In the study of real number sequences (aâ‚™)â‚™âˆˆâ„•, convergence has a single, clear definition: aâ‚™ â†’ a as n â†’ âˆ if for every Îµ > 0 there exists a positive integer N such that the sequence after N is always within Îµ of the limit a. However, when we turn our attention to sequences of functionsâ€”particularly random variablesâ€”the notion of convergence becomes considerably more nuanced. This note explores several fundamental modes of convergence and their relationships in probability theory.

Throughout our discussion, we fix a probability space Î© and consider a sequence of random variables (Xâ‚™)â‚™âˆˆâ„•, along with another random variable X.

## Almost Sure Convergence

The sequence (Xâ‚™)â‚™âˆˆâ„• converges almost surely (a.s.) or with probability one to the limit X if the set of outcomes Ï‰ âˆˆ Î© for which Xâ‚™(Ï‰) â†’ X(Ï‰) forms an event of probability one. This means that all observed realizations of the sequence converge to the limit. We denote this mode of convergence by Xâ‚™ â†’^{a.s.} X as n â†’ âˆ.

While one could define an even stronger notion requiring Xâ‚™(Ï‰) â†’ X(Ï‰) for every outcome (not just for a set of probability one), probabilists disregard events of probability zero as they are never observed. Thus, almost sure convergence is regarded as the strongest practical form of convergence.

### Theorem 1 (Strong Law of Large Numbers)
If (Xâ‚™)â‚™âˆˆâ„• are pairwise independent and identically distributed with E[|Xâ‚|] < âˆ, then:

nâ»Â¹ Î£áµ¢â‚Œâ‚â¿ Xáµ¢ â†’^{a.s.} E[Xâ‚] as n â†’ âˆ

This celebrated result states that sample averages of identically distributed random variables converge almost surely to their expected value under very weak assumptions.

Applications of almost sure convergence appear throughout probability theory: in discrete-time Markov chains (analyzing the fraction of time spent in states), in information theory's asymptotic equipartition property, and in machine learning's stochastic gradient descent algorithms.

## Convergence in Probability

The sequence (Xâ‚™)â‚™âˆˆâ„• converges in probability to X, denoted Xâ‚™ â†’^P X as n â†’ âˆ, if for every Îµ > 0:

P(|Xâ‚™ - X| > Îµ) â†’ 0 as n â†’ âˆ

This means that for any fixed Îµ > 0, the probability that the sequence deviates from the supposed limit X by more than Îµ becomes vanishingly small.

### Theorem 2
If Xâ‚™ â†’^{a.s.} X as n â†’ âˆ, then Xâ‚™ â†’^P X as n â†’ âˆ.

**Proof:** Fix Îµ > 0. Define Aâ‚™ := â‹ƒâ‚˜â‚Œâ‚™^âˆ {|Xâ‚˜ - X| > Îµ} to be the event that at least one of Xâ‚™, Xâ‚™â‚Šâ‚, ... deviates from X by more than Îµ. Observe that Aâ‚ âŠ‡ Aâ‚‚ âŠ‡ ... decreases to an event A which has probability zero, since the almost sure convergence of (Xâ‚™)â‚™âˆˆâ„• implies that for all outcomes Ï‰ (outside of an event of probability zero), the sequence (|Xâ‚™(Ï‰) - X(Ï‰)|)â‚™âˆˆâ„• is eventually bounded by Îµ. Thus:

P(|Xâ‚™ - X| > Îµ) â‰¤ P(Aâ‚™) â†’ P(A) = 0 as n â†’ âˆ

However, the converse is not true.

### Example 1
Consider Xâ‚™ = 0 for all n âˆˆ â„•. For each j âˆˆ â„•, pick an index Nâ±¼ uniformly at random from {2Ê², ..., 2Ê²âºÂ¹ - 1} and set X_{Nâ±¼} = 1. We have:

P(Xâ‚™ â‰  0) = P(N_{âŒŠlogâ‚‚ nâŒ‹} = n) = 2^{-âŒŠlogâ‚‚ nâŒ‹} â†’ 0 as n â†’ âˆ

So Xâ‚™ â†’^P 0. However, for any Ï‰ âˆˆ Î©, the sequence (Xâ‚™(Ï‰))â‚™âˆˆâ„• takes on both values 0 and 1 infinitely often, so (Xâ‚™)â‚™âˆˆâ„• does not converge almost surely.

As a consequence of Theorems 1 and 2, if (Xâ‚™)â‚™âˆˆâ„• are pairwise independent and identically distributed with E[|Xâ‚|] < âˆ, then nâ»Â¹ Î£áµ¢â‚Œâ‚â¿ Xáµ¢ â†’^P E[Xâ‚], known as the Weak Law of Large Numbers (WLLN).

The practical distinction: with convergence in probability, deviations of any particular size become increasingly rare but may occur forever. With almost sure convergence, for any observed realization, there will come a time after which no such deviations occur.

## Convergence in Distribution

The sequence (Xâ‚™)â‚™âˆˆâ„• converges in distribution or in law to X, denoted Xâ‚™ â†’^d X as n â†’ âˆ, if for each x âˆˆ â„ such that P(X = x) = 0:

P(Xâ‚™ â‰¤ x) â†’ P(X â‰¤ x) as n â†’ âˆ

Unlike the previous forms, convergence in distribution does not require all random variables to be defined on the same probability space.

### Example 2
Consider the constant random variables Xâ‚™ := 2â»â¿ and X := 0. We have P(Xâ‚™ â‰¤ 0) = 0 for all n âˆˆ â„•, whereas P(X â‰¤ 0) = 1, so P(Xâ‚™ â‰¤ 0) does not converge to P(X â‰¤ 0). Since P(X = 0) = 1, we fix this by only looking at points x where P(X = x) = 0, i.e., where the CDF of X is continuous.

### Theorem 3
1. If (Xâ‚™)â‚™âˆˆâ„• and X take values in â„¤, and if P(Xâ‚™ = x) â†’ P(X = x) for all x âˆˆ â„¤, then Xâ‚™ â†’^d X.

2. If (Xâ‚™)â‚™âˆˆâ„• and X are continuous random variables, and if f_{Xâ‚™}(x) â†’ f_X(x) for all x âˆˆ â„, then Xâ‚™ â†’^d X.

### Theorem 4
If Xâ‚™ â†’^P X as n â†’ âˆ, then Xâ‚™ â†’^d X as n â†’ âˆ.

**Proof:** Let x be a point such that P(X = x) = 0. Fix Îµ > 0. We can write:

P(Xâ‚™ â‰¤ x) = P(Xâ‚™ â‰¤ x, |Xâ‚™ - X| < Îµ) + P(Xâ‚™ â‰¤ x, |Xâ‚™ - X| â‰¥ Îµ)
         â‰¤ P(X â‰¤ x + Îµ) + P(|Xâ‚™ - X| â‰¥ Îµ)

Similarly:

P(X â‰¤ x - Îµ) = P(X â‰¤ x - Îµ, |Xâ‚™ - X| < Îµ) + P(X â‰¤ x - Îµ, |Xâ‚™ - X| â‰¥ Îµ)
             â‰¤ P(Xâ‚™ â‰¤ x) + P(|Xâ‚™ - X| â‰¥ Îµ)

Combining the bounds:

P(X â‰¤ x - Îµ) - P(|Xâ‚™ - X| â‰¥ Îµ) â‰¤ P(Xâ‚™ â‰¤ x) â‰¤ P(X â‰¤ x + Îµ) + P(|Xâ‚™ - X| â‰¥ Îµ)

Since Xâ‚™ â†’^P X, then P(|Xâ‚™ - X| â‰¥ Îµ) â†’ 0, so eventually (P(Xâ‚™ â‰¤ x))â‚™âˆˆâ„• is trapped between P(X â‰¤ x - Îµ) and P(X â‰¤ x + Îµ). This holds for all Îµ > 0 and the CDF of X is continuous at x, so taking Îµ â†’ 0, we conclude P(Xâ‚™ â‰¤ x) â†’ P(X â‰¤ x).

The converse is not true: convergence in distribution does not imply convergence in probability.

### Example 3
Let Xâ‚€ ~ Uniform[-1, 1], and for each positive integer n, let Xâ‚™ := (-1)â¿Xâ‚€. Then Xâ‚™ ~ Uniform[-1, 1] for all n âˆˆ â„• because the Uniform[-1, 1] distribution is symmetric around the origin, so convergence in distribution holds trivially. However, (Xâ‚™)â‚™âˆˆâ„• does not converge in probability.

Thus we have established the hierarchy:

Xâ‚™ â†’^{a.s.} X  âŸ¹  Xâ‚™ â†’^P X  âŸ¹  Xâ‚™ â†’^d X

### Theorem 5 (Central Limit Theorem)
If (Xâ‚™)â‚™âˆˆâ„• is a sequence of i.i.d. random variables with common mean Î¼ and finite variance ÏƒÂ², then:

(Î£áµ¢â‚Œâ‚â¿ Xáµ¢ - nÎ¼)/(Ïƒâˆšn) â†’^d Z as n â†’ âˆ

where Z is a standard Gaussian random variable. Explicitly, for all x âˆˆ â„:

P((Î£áµ¢â‚Œâ‚â¿ Xáµ¢ - nÎ¼)/(Ïƒâˆšn) â‰¤ x) â†’ âˆ«â‚‹âˆË£ (1/âˆš2Ï€) exp(-zÂ²/2) dz as n â†’ âˆ

The CLT plays a crucial role in statistics for providing asymptotic confidence intervals. Statisticians also work to prove convergence in distribution to other common distributions such as chi-squared or t distributions.

### Theorem 6 (Poisson Law of Rare Events)
If Xâ‚™ ~ Binomial(n, pâ‚™) where pâ‚™ â†’ 0 such that npâ‚™ â†’ Î» > 0, then Xâ‚™ â†’^d X, where X ~ Poisson(Î»).

Many situations involving balls and bins have Poisson limits, making this result useful in random graph models.

## Miscellaneous Results

### Continuous Mapping

#### Theorem 7 (Continuous Mapping)
Let f be a continuous function.

1. If Xâ‚™ â†’^{a.s.} X, then f(Xâ‚™) â†’^{a.s.} f(X).
2. If Xâ‚™ â†’^P X, then f(Xâ‚™) â†’^P f(X).
3. If Xâ‚™ â†’^d X, then f(Xâ‚™) â†’^d f(X).

A typical application involves analyzing sequences through log or exp transformations when showing convergence of (log Xâ‚™)â‚™âˆˆâ„• or (exp Xâ‚™)â‚™âˆˆâ„• is easier than the original sequence.

### Convergence of Expectation

In general, none of the above modes of convergence imply that E[Xâ‚™] â†’ E[X]. For example, let U ~ Uniform[0, 1] and let Xâ‚™ := nğŸ™{U â‰¤ nâ»Â¹}. Then Xâ‚™ â†’^{a.s.} 0, but E[Xâ‚™] = 1 for all n.

#### Theorem 8 (Convergence Theorems)
Suppose Xâ‚™ â†’^{a.s.} X.

1. **Monotone Convergence:** If 0 â‰¤ Xâ‚ â‰¤ Xâ‚‚ â‰¤ Xâ‚ƒ â‰¤ ..., then E[Xâ‚™] â†’ E[X].

2. **Dominated Convergence:** If there exists a random variable Y â‰¥ 0 with E[Y] < âˆ and |Xâ‚™|, |X| â‰¤ Y for all n, then E[Xâ‚™] â†’ E[X].

These convergence theorems are technical tools used in advanced probability theory to justify convergence of expectations.