{
  "id": "msg_01FQMVq39f2v2y1P2Jtt7jVe",
  "type": "message",
  "role": "assistant",
  "model": "claude-opus-4-1-20250805",
  "content": [
    {
      "type": "text",
      "text": "# Chapter 6: Dynamic programming\n\n## 6.1 Shortest paths in dags, revisited\n\nDynamic programming represents one of the most powerful algorithmic paradigms, alongside linear programming, offering broad applicability when specialized methods fail. While this generality often comes at a cost in efficiency, the technique proves invaluable for solving complex optimization problems.\n\nThe foundation of dynamic programming lies in exploiting the special structure of directed acyclic graphs (dags). In a dag, nodes can be linearized—arranged so all edges go from left to right. This property makes shortest path computation particularly straightforward.\n\nFor any node D with predecessors B and C, the shortest path satisfies:\ndist(D) = min{dist(B) + 1, dist(C) + 3}\n\nThis relationship extends to all nodes, enabling computation of all distances in a single pass through the linearized order. The algorithm solves a collection of subproblems {dist(u) : u ∈ V}, starting with the smallest (dist(s) = 0) and progressively tackling larger ones.\n\nThe key insight: at each node, we compute some function of the values of the node's predecessors. This technique generalizes beyond shortest paths—we could compute longest paths, paths with smallest products, or other optimizations simply by changing the function.\n\nDynamic programming fundamentally involves identifying and solving subproblems from smallest to largest, using answers to small problems to solve larger ones. The dag is implicit: nodes represent subproblems, edges represent dependencies between them.\n\n## 6.2 Longest increasing subsequences\n\nGiven a sequence a₁, ..., aₙ, an increasing subsequence consists of elements aᵢ₁, aᵢ₂, ..., aᵢₖ where i₁ < i₂ < ... < iₖ and the numbers are strictly increasing. The task: find the increasing subsequence of greatest length.\n\nTo understand the solution space, we create a graph with node i for each element aᵢ, adding directed edges (i, j) whenever i < j and aᵢ < aⱼ. This graph forms a dag where increasing subsequences correspond to paths.\n\nThe algorithm defines L(j) as the length of the longest increasing subsequence ending at j:\n\n```\nfor j = 1, 2, ..., n:\n    L(j) = 1 + max{L(i) : (i, j) ∈ E}\nreturn maxⱼ L(j)\n```\n\nThis exemplifies the dynamic programming property: an ordering on subproblems with a relation showing how to solve each subproblem given answers to \"smaller\" ones. The computation takes O(n²) time.\n\nThe danger of recursion becomes apparent here. A naive recursive approach would solve the same subproblems repeatedly, leading to exponential time complexity. Dynamic programming achieves efficiency by explicitly enumerating distinct subproblems and solving them in the right order.\n\n## 6.3 Edit distance\n\nEdit distance measures the similarity between strings through alignment. An alignment writes strings one above the other, with gaps indicated by \"−\". The cost equals the number of differing columns, and edit distance is the minimum cost over all possible alignments.\n\nFor strings x[1···m] and y[1···n], we define subproblems:\nE(i, j) = edit distance between x[1···i] and y[1···j]\n\nThe rightmost column of any alignment must be one of three forms, leading to:\nE(i, j) = min{1 + E(i−1, j), 1 + E(i, j−1), diff(i, j) + E(i−1, j−1)}\n\nwhere diff(i, j) = 0 if x[i] = y[j], and 1 otherwise.\n\nThe algorithm fills an m×n table with base cases E(0, j) = j and E(i, 0) = i:\n\n```\nfor i = 0, 1, ..., m:\n    E(i, 0) = i\nfor j = 1, 2, ..., n:\n    E(0, j) = j\nfor i = 1, 2, ..., m:\n    for j = 1, 2, ..., n:\n        E(i, j) = min{E(i−1, j) + 1, E(i, j−1) + 1, E(i−1, j−1) + diff(i, j)}\nreturn E(m, n)\n```\n\nRunning time: O(mn).\n\nThe underlying dag structure reveals itself with nodes corresponding to positions (i, j) and edges representing precedence constraints. By weighting edges appropriately, edit distances become shortest paths in this dag.\n\n## 6.4 Knapsack\n\nThe knapsack problem involves selecting items of weights w₁, ..., wₙ and values v₁, ..., vₙ to maximize value while staying within capacity W.\n\n### Knapsack with repetition\n\nDefine K(w) = maximum value achievable with capacity w.\n\nIf the optimal solution includes item i, then:\nK(w) = max{K(w − wᵢ) + vᵢ : wᵢ ≤ w}\n\nThe algorithm:\n```\nK(0) = 0\nfor w = 1 to W:\n    K(w) = max{K(w − wᵢ) + vᵢ : wᵢ ≤ w}\nreturn K(W)\n```\n\nTime complexity: O(nW).\n\n### Knapsack without repetition\n\nWhen repetitions are forbidden, we need additional information about items used. Define:\nK(w, j) = maximum value using capacity w and items 1, ..., j\n\nThe recurrence:\nK(w, j) = max{K(w − wⱼ, j−1) + vⱼ, K(w, j−1)}\n\nThe algorithm fills a (W+1) × (n+1) table in O(nW) time.\n\nMemoization offers an alternative: a recursive implementation that remembers previous invocations via hash tables. While achieving the same O(nW) complexity, memoization only solves actually-needed subproblems, potentially saving computation when many subproblems are irrelevant.\n\n## 6.5 Chain matrix multiplication\n\nMultiplying matrices A₁ × A₂ × ... × Aₙ requires choosing a parenthesization. Different orders yield dramatically different costs.\n\nDefine C(i, j) = minimum cost of multiplying Aᵢ × ... × Aⱼ.\n\nFor the optimal subtree, splitting at position k:\nC(i, j) = min{C(i, k) + C(k+1, j) + mᵢ₋₁ · mₖ · mⱼ : i ≤ k < j}\n\nThe algorithm:\n```\nfor i = 1 to n:\n    C(i, i) = 0\nfor s = 1 to n−1:\n    for i = 1 to n−s:\n        j = i + s\n        C(i, j) = min{C(i, k) + C(k+1, j) + mᵢ₋₁ · mₖ · mⱼ : i ≤ k < j}\nreturn C(1, n)\n```\n\nTime complexity: O(n³).\n\n## 6.6 Shortest paths\n\n### Shortest reliable paths\n\nWhen path reliability matters, we seek shortest paths using at most k edges. Define:\ndist(v, i) = length of shortest path from s to v using i edges\n\nThe update equation:\ndist(v, i) = min{dist(u, i−1) + ℓ(u, v) : (u, v) ∈ E}\n\n### All-pairs shortest paths\n\nThe Floyd-Warshall algorithm finds shortest paths between all vertex pairs. Define:\ndist(i, j, k) = shortest path from i to j using only vertices {1, ..., k} as intermediates\n\nThe key insight: paths through k either use k or don't:\ndist(i, j, k) = min{dist(i, k, k−1) + dist(k, j, k−1), dist(i, j, k−1)}\n\nTime complexity: O(|V|³).\n\n### The traveling salesman problem\n\nFor cities {1, ..., n}, find the minimum-length tour visiting each exactly once.\n\nDefine C(S, j) = shortest path visiting each node in S exactly once, starting at 1 and ending at j.\n\nRecurrence:\nC(S, j) = min{C(S−{j}, i) + dᵢⱼ : i ∈ S, i ≠ j}\n\nTime complexity: O(n²2ⁿ)—exponential but much better than the O(n!) brute force.\n\n## 6.7 Independent sets in trees\n\nAn independent set contains no adjacent nodes. While finding maximum independent sets is intractable in general graphs, trees admit linear-time solutions.\n\nAfter rooting the tree at r, define:\nI(u) = size of largest independent set in subtree rooted at u\n\nThe recurrence considers whether u is included:\nI(u) = max{1 + Σ(grandchildren w) I(w), Σ(children w) I(w)}\n\nThe algorithm processes nodes bottom-up in the rooted tree, achieving O(|V| + |E|) time.\n\nMemory optimization is crucial in dynamic programming. Often we need only remember values until larger dependent subproblems are solved, allowing substantial space savings compared to storing all subproblem solutions."
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 21015,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "output_tokens": 2295,
    "service_tier": "standard"
  }
}