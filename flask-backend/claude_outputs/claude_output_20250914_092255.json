{
  "id": "msg_015MzJzz1o64dbouebbcpnvo",
  "type": "message",
  "role": "assistant",
  "model": "claude-opus-4-1-20250805",
  "content": [
    {
      "type": "text",
      "text": "# Reversible Markov Chains\n\n## 1 Reversibility\n\nConsider an irreducible Markov chain (Xₙ)ₙ∈ℕ on the finite state space X with transition probability matrix P. When is it the case that the Markov chain \"looks the same\" regardless of whether we run it forwards in time or backwards in time? Formally, fix a positive integer N and define the reversed chain Yₙ := X_{N-n} for n = 0, 1, ..., N. Then, (Y₀, ..., Y_N) = (X_N, ..., X₀), so (Yₙ)ⁿ₌₀ᴺ is the sequence of states we observe if, starting at time N, we run the original Markov chain \"backwards\". To justify the name \"reversed chain\", we prove:\n\n**Theorem 1.** If the irreducible Markov chain (Xₙ)ₙ∈ℕ is started from the stationary distribution π, then the reversed chain (Yₙ)ⁿ₌₀ᴺ is an irreducible Markov chain with transition probabilities P̂(x, y) = π(y)P(y, x)/π(x) for x, y ∈ X. The stationary distribution for the reversed chain is also π.\n\n**Proof.** For a positive integer k < N and a feasible sequence of states x₀, x₁, ..., xₖ, xₖ₊₁ ∈ X,\n\nP(Yₖ₊₁ = xₖ₊₁ | Yₖ = xₖ, ..., Y₁ = x₁, Y₀ = x₀)\n= P(X_{N-k-1} = xₖ₊₁ | X_{N-k} = xₖ, ..., X_{N-1} = x₁, X_N = x₀)\n\n(now use the \"backwards Markov property\")\n\n= P(X_{N-k-1} = xₖ₊₁ | X_{N-k} = xₖ) = P(X_{N-k-1} = xₖ₊₁, X_{N-k} = xₖ)/P(X_{N-k} = xₖ)\n\n= π(xₖ₊₁)P(xₖ₊₁, xₖ)/π(xₖ).\n\nTherefore, (Yₙ)ⁿ₌₀ᴺ is a Markov chain and the transition probabilities are P̂(x, y) := π(y)P(y, x)/π(x) for x, y ∈ X. Irreducibility of (Yₙ)ⁿ₌₀ᴺ follows from irreducibility of the original chain (you are encouraged to think about why this is true). Finally, since P(Xₙ = x) = π(x) for all n ∈ ℕ (since the original chain is started from stationarity), then P(Yₙ = x) = π(x) for all n = 0, 1, ..., N, which implies that π is the stationary distribution for the reversed chain.\n\nNow, to answer the question we posed above: when does the reversed chain look the same as the original chain? We need the transition probabilities to be the same in both chains, i.e., P̂(x, y) = P(x, y) for all x, y ∈ X, or equivalently,\n\nπ(x)P(x, y) = π(y)P(y, x) for all x, y ∈ X.     (1)\n\nA Markov chain whose stationary distribution π and transition probability matrix P satisfy (1) is called **reversible**.\n\nPerhaps surprisingly, the notion of reversibility is more than just a mathematical curiosity. Reversibility finds applications in the design of queueing systems and sampling algorithms; we will soon discuss the latter in more detail. We give an example of the former:\n\n**Example 1.** Consider a discrete-time queue: at each n ∈ ℕ, a customer arrives with probability p (this is called a Bernoulli process), and if there are any customers in the queue, then one of the customers is served with probability q. The arrivals and services are all assumed to be mutually independent. Then, the length of the queue is a Markov chain, and in fact it turns out to be reversible. Notice though that when we look at the queue backwards in time, then the customer departures become arrivals, which implies that the departure process is also a Bernoulli process!\n\n### 1.1 Detailed Balance\n\nThe equations (1) are also called the **detailed balance equations**. In general, the condition for stationarity is π(y) = Σₓ∈X π(x)P(x, y) for all y ∈ X. Imagine that a total mass of π(y) sits at state y, and when the Markov chain undergoes a transition, the total mass of π(y) leaves state y and is divided up among the neighbors of y according to the transition probabilities P(y, ·). The balance equations then express the global condition that the total mass leaving state y equals the sum of the mass entering y. On the other hand, the detailed balance equations reflect a local condition that the mass exchanged along each edge (x, y) is balanced; this is a stronger condition than global balance.\n\n**Exercise 1.** Consider an irreducible Markov chain on the finite state space X with transition probability matrix P.\n\n1. Suppose that P̃ is another transition probability matrix of the same dimensions as P (the rows sum to 1). Show that if π is a probability distribution on X which satisfies π(x)P(x, y) = π(y)P̃(y, x) for all x, y ∈ X, then π is the stationary distribution and P̃ is the transition probability matrix of the reversed chain.\n\n2. In particular, if π satisfies the detailed balance equations (1), then π is the stationary distribution.\n\nTo summarize the situation, the detailed balance equations are sufficient for stationarity but not necessary; there exist Markov chains whose stationary distributions do not satisfy detailed balance.¹ However, it is useful to try detailed balance first, since solving (1) is usually easier than solving the general balance equations. To take full advantage of the power of this tool, it is helpful to know the following result:\n\n**Exercise 2.** The graph associated with a Markov chain is formed by taking the transition diagram of the chain, forgetting the directions of all of the edges, removing multiple edges, and removing self-loops. If the graph of a finite-state irreducible Markov chain is a tree, then the stationary distribution of the Markov chain satisfies detailed balance.\n\nIn particular, Markov chains which look like a line satisfy detailed balance: for example, a random walk which can only move to the immediate left or to the immediate right. This includes the queue in Example 1.\n\n**Example 2 (Metropolis-Hastings).** Often in applications, we encounter probability distributions that are computationally intractable, but if we can design algorithms which produce samples from the desired distribution, then we can infer important properties about the distribution (such as the expectation or the mode). **Monte Carlo Markov Chain (MCMC)** refers to class of algorithms which produce samples from a distribution by designing a Markov chain where the given distribution is stationary for the chain. After performing transitions until the chain reaches stationarity, we can then take samples from the chain to approximate the given distribution.\n\nAn important MCMC algorithm, called the **Metropolis-Hastings algorithm**, only requires knowledge about the desired distribution up to a constant factor. (The constant factor is important—it is often the reason why the probability distribution is computationally intractable!) Crucially, the Metropolis-Hastings chain satisfies detailed balance.\n\n---\n\n¹If you are interested, necessary and sufficient conditions for detailed balance are given by the Kolmogorov cycle condition."
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 2207,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "output_tokens": 1950,
    "service_tier": "standard"
  }
}