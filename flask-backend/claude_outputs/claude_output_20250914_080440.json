{
  "id": "msg_019r2qcJB1bQgwXtL5g8gtEu",
  "type": "message",
  "role": "assistant",
  "model": "claude-opus-4-1-20250805",
  "content": [
    {
      "type": "text",
      "text": "# Chapter 2 · Divide-and-conquer algorithms\n\nThe divide-and-conquer strategy solves a problem by:\n1. Breaking it into subproblems that are themselves smaller instances of the same type of problem\n2. Recursively solving these subproblems  \n3. Appropriately combining their answers\n\nThe real work is done piecemeal, in three different places: in the partitioning of problems into subproblems; at the very tail end of the recursion, when the subproblems are so small that they are solved outright; and in the gluing together of partial answers. These are held together and coordinated by the algorithm's core recursive structure.\n\nAs an introductory example, we'll see how this technique yields a new algorithm for multiplying numbers, one that is much more efficient than the method we all learned in elementary school!\n\n## 2.1 Multiplication\n\nThe mathematician Carl Friedrich Gauss (1777–1855) once noticed that although the product of two complex numbers\n\n(a + bi)(c + di) = ac - bd + (bc + ad)i\n\nseems to involve four real-number multiplications, it can in fact be done with just three: ac, bd, and (a + b)(c + d), since\n\nbc + ad = (a + b)(c + d) - ac - bd.\n\nIn our big-O way of thinking, reducing the number of multiplications from four to three seems wasted ingenuity. But this modest improvement becomes very significant when applied recursively.\n\nLet's move away from complex numbers and see how this helps with regular multiplication. Suppose x and y are two n-bit integers, and assume for convenience that n is a power of 2 (the more general case is hardly any different). As a first step toward multiplying x and y, split each of them into their left and right halves, which are n/2 bits long:\n\nx = xL xR = 2^(n/2)xL + xR\ny = yL yR = 2^(n/2)yL + yR.\n\nFor instance, if x = 10110110₂ (the subscript 2 means \"binary\") then xL = 1011₂, xR = 0110₂, and x = 1011₂ × 2⁴ + 0110₂. The product of x and y can then be rewritten as\n\nxy = (2^(n/2)xL + xR)(2^(n/2)yL + yR) = 2^n xLyL + 2^(n/2)(xLyR + xRyL) + xRyR.\n\nWe will compute xy via the expression on the right. The additions take linear time, as do the multiplications by powers of 2 (which are merely left-shifts). The significant operations are the four n/2-bit multiplications, xLyL, xLyR, xRyL, xRyR; these we can handle by four recursive calls. Thus our method for multiplying n-bit numbers starts by making recursive calls to multiply these four pairs of n/2-bit numbers (four subproblems of half the size), and then evaluates the preceding expression in O(n) time. Writing T(n) for the overall running time on n-bit inputs, we get the recurrence relation\n\nT(n) = 4T(n/2) + O(n).\n\nWe will soon see general strategies for solving such equations. In the meantime, this particular one works out to O(n²), the same running time as the traditional grade-school multiplication technique. So we have a radically new algorithm, but we haven't yet made any progress in efficiency. How can our method be sped up?\n\nThis is where Gauss's trick comes to mind. Although the expression for xy seems to demand four n/2-bit multiplications, as before just three will do: xLyL, xRyR, and (xL+xR)(yL+yR), since xLyR+xRyL = (xL+xR)(yL+yR)−xLyL−xRyR. The resulting algorithm, shown in Figure 2.1, has an improved running time of\n\nT(n) = 3T(n/2) + O(n).\n\nThe point is that now the constant factor improvement, from 4 to 3, occurs at every level of the recursion, and this compounding effect leads to a dramatically lower time bound of O(n^1.59).\n\n## 2.2 Recurrence relations\n\nDivide-and-conquer algorithms often follow a generic pattern: they tackle a problem of size n by recursively solving, say, a subproblems of size n/b and then combining these answers in O(n^d) time, for some a, b, d > 0 (in the multiplication algorithm, a = 3, b = 2, and d = 1). Their running time can therefore be captured by the equation T(n) = aT(⌈n/b⌉) + O(n^d). We next derive a closed-form solution to this general recurrence so that we no longer have to solve it explicitly in each new instance.\n\n**Master theorem** If T(n) = aT(⌈n/b⌉) + O(n^d) for some constants a > 0, b > 1, and d ≥ 0, then\n\nT(n) = \n- O(n^d) if d > log_b a\n- O(n^d log n) if d = log_b a  \n- O(n^(log_b a)) if d < log_b a\n\nThis single theorem tells us the running times of most of the divide-and-conquer procedures we are likely to use.\n\n## 2.3 Mergesort\n\nThe problem of sorting a list of numbers lends itself immediately to a divide-and-conquer strategy: split the list into two halves, recursively sort each half, and then merge the two sorted sublists.\n\nThe correctness of this algorithm is self-evident, as long as a correct merge subroutine is specified. If we are given two sorted arrays x[1...k] and y[1...l], how do we efficiently merge them into a single sorted array z[1...k+l]? Well, the very first element of z is either x[1] or y[1], whichever is smaller. The rest of z[·] can then be constructed recursively.\n\nThis merge procedure does a constant amount of work per recursive call (provided the required array space is allocated in advance), for a total running time of O(k + l). Thus merges are linear, and the overall time taken by mergesort is\n\nT(n) = 2T(n/2) + O(n),\n\nor O(n log n).\n\n## 2.4 Medians\n\nThe median of a list of numbers is its 50th percentile: half the numbers are bigger than it, and half are smaller. For instance, the median of [45, 1, 10, 30, 25] is 25, since this is the middle element when the numbers are arranged in order. If the list has even length, there are two choices for what the middle element could be, in which case we pick the smaller of the two, say.\n\nComputing the median of n numbers is easy: just sort them. The drawback is that this takes O(n log n) time, whereas we would ideally like something linear. We have reason to be hopeful, because sorting is doing far more work than we really need—we just want the middle element and don't care about the relative ordering of the rest of them.\n\nWhen looking for a recursive solution, it is paradoxically often easier to work with a more general version of the problem—for the simple reason that this gives a more powerful step to recurse upon. In our case, the generalization we will consider is selection.\n\nSELECTION\nInput: A list of numbers S; an integer k\nOutput: The kth smallest element of S\n\n## 2.5 Matrix multiplication\n\nThe product of two n × n matrices X and Y is a third n × n matrix Z = XY, with (i, j)th entry\n\nZ_ij = Σ(k=1 to n) X_ik Y_kj.\n\nIn general, XY is not the same as YX; matrix multiplication is not commutative.\n\nThe preceding formula implies an O(n³) algorithm for matrix multiplication: there are n² entries to be computed, and each takes O(n) time. For quite a while, this was widely believed to be the best running time possible. It was therefore a source of great excitement when in 1969, the German mathematician Volker Strassen announced a significantly more efficient algorithm, based upon divide-and-conquer.\n\n## 2.6 The fast Fourier transform\n\nWe have so far seen how divide-and-conquer gives fast algorithms for multiplying integers and matrices; our next target is polynomials. The product of two degree-d polynomials is a polynomial of degree 2d, for example:\n\n(1 + 2x + 3x²) · (2 + x + 4x²) = 2 + 5x + 12x² + 11x³ + 12x⁴.\n\nThe solution we will develop, the fast Fourier transform, has revolutionized—indeed, defined—the field of signal processing. Because of its huge importance, and its wealth of insights from different fields of study, we will approach it a little more leisurely than usual."
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 25196,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "output_tokens": 2201,
    "service_tier": "standard"
  }
}