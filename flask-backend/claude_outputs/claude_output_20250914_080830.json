{
  "id": "msg_01Vq6RR93mrzchhJGqVKhkGP",
  "type": "message",
  "role": "assistant",
  "model": "claude-opus-4-1-20250805",
  "content": [
    {
      "type": "text",
      "text": "# Chapter 2: Divide-and-conquer algorithms\n\nThe divide-and-conquer strategy solves a problem by:\n1. Breaking it into subproblems that are themselves smaller instances of the same type of problem\n2. Recursively solving these subproblems  \n3. Appropriately combining their answers\n\nThe real work is done piecemeal, in three different places: in the partitioning of problems into subproblems; at the very tail end of the recursion, when the subproblems are so small that they are solved outright; and in the gluing together of partial answers. These are held together and coordinated by the algorithm's core recursive structure.\n\nAs an introductory example, we'll see how this technique yields a new algorithm for multiplying numbers, one that is much more efficient than the method we all learned in elementary school!\n\n## 2.1 Multiplication\n\nThe mathematician Carl Friedrich Gauss (1777–1855) once noticed that although the product of two complex numbers\n\n(a + bi)(c + di) = ac - bd + (bc + ad)i\n\nseems to involve four real-number multiplications, it can in fact be done with just three: ac, bd, and (a + b)(c + d), since\n\nbc + ad = (a + b)(c + d) - ac - bd.\n\nIn our big-O way of thinking, reducing the number of multiplications from four to three seems wasted ingenuity. But this modest improvement becomes very significant when applied recursively.\n\nLet's move away from complex numbers and see how this helps with regular multiplication. Suppose x and y are two n-bit integers, and assume for convenience that n is a power of 2 (the more general case is hardly any different). As a first step toward multiplying x and y, split each of them into their left and right halves, which are n/2 bits long:\n\nx = 2^(n/2)x_L + x_R  \ny = 2^(n/2)y_L + y_R\n\nFor instance, if x = 10110110₂ (the subscript 2 means \"binary\") then x_L = 1011₂, x_R = 0110₂, and x = 1011₂ × 2⁴ + 0110₂. The product of x and y can then be rewritten as\n\nxy = (2^(n/2)x_L + x_R)(2^(n/2)y_L + y_R) = 2^n x_Ly_L + 2^(n/2)(x_Ly_R + x_Ry_L) + x_Ry_R.\n\nWe will compute xy via the expression on the right. The additions take linear time, as do the multiplications by powers of 2 (which are merely left-shifts). The significant operations are the four n/2-bit multiplications, x_Ly_L, x_Ly_R, x_Ry_L, x_Ry_R; these we can handle by four recursive calls. Thus our method for multiplying n-bit numbers starts by making recursive calls to multiply these four pairs of n/2-bit numbers (four subproblems of half the size), and then evaluates the preceding expression in O(n) time. Writing T(n) for the overall running time on n-bit inputs, we get the recurrence relation\n\nT(n) = 4T(n/2) + O(n).\n\nWe will soon see general strategies for solving such equations. In the meantime, this particular one works out to O(n²), the same running time as the traditional grade-school multiplication technique. So we have a radically new algorithm, but we haven't yet made any progress in efficiency. How can our method be sped up?\n\nThis is where Gauss's trick comes to mind. Although the expression for xy seems to demand four n/2-bit multiplications, as before just three will do: x_Ly_L, x_Ry_R, and (x_L+x_R)(y_L+y_R), since x_Ly_R+x_Ry_L = (x_L+x_R)(y_L+y_R) - x_Ly_L - x_Ry_R. The resulting algorithm, shown in Figure 2.1, has an improved running time of\n\nT(n) = 3T(n/2) + O(n).\n\nThe point is that now the constant factor improvement, from 4 to 3, occurs at every level of the recursion, and this compounding effect leads to a dramatically lower time bound of O(n^1.59).\n\n## 2.2 Recurrence relations\n\nDivide-and-conquer algorithms often follow a generic pattern: they tackle a problem of size n by recursively solving, say, a subproblems of size n/b and then combining these answers in O(n^d) time, for some a, b, d > 0 (in the multiplication algorithm, a = 3, b = 2, and d = 1). Their running time can therefore be captured by the equation T(n) = aT(⌈n/b⌉) + O(n^d). We next derive a closed-form solution to this general recurrence so that we no longer have to solve it explicitly in each new instance.\n\n**Master theorem** If T(n) = aT(⌈n/b⌉) + O(n^d) for some constants a > 0, b > 1, and d ≥ 0, then\n\nT(n) = {\n  O(n^d)         if d > log_b a\n  O(n^d log n)   if d = log_b a  \n  O(n^(log_b a)) if d < log_b a\n}\n\nThis single theorem tells us the running times of most of the divide-and-conquer procedures we are likely to use.\n\n### Binary search\n\nThe ultimate divide-and-conquer algorithm is, of course, binary search: to find a key k in a large file containing keys z[0, 1, ..., n - 1] in sorted order, we first compare k with z[n/2], and depending on the result we recurse either on the first half of the file, z[0, ..., n/2 - 1], or on the second half, z[n/2, ..., n - 1]. The recurrence now is T(n) = T(⌈n/2⌉) + O(1), which is the case a = 1, b = 2, d = 0. Plugging into our master theorem we get the familiar solution: a running time of just O(log n).\n\n## 2.3 Mergesort\n\nThe problem of sorting a list of numbers lends itself immediately to a divide-and-conquer strategy: split the list into two halves, recursively sort each half, and then merge the two sorted sublists.\n\nThe correctness of this algorithm is self-evident, as long as a correct merge subroutine is specified. If we are given two sorted arrays x[1...k] and y[1...l], how do we efficiently merge them into a single sorted array z[1...k + l]? Well, the very first element of z is either x[1] or y[1], whichever is smaller. The rest of z[·] can then be constructed recursively.\n\nLooking back at the mergesort algorithm, we see that all the real work is done in merging, which doesn't start until the recursion gets down to singleton arrays. The singletons are merged in pairs, to yield arrays with two elements. Then pairs of these 2-tuples are merged, producing 4-tuples, and so on.\n\n## 2.4 Medians\n\nThe median of a list of numbers is its 50th percentile: half the numbers are bigger than it, and half are smaller. For instance, the median of [45, 1, 10, 30, 25] is 25, since this is the middle element when the numbers are arranged in order. If the list has even length, there are two choices for what the middle element could be, in which case we pick the smaller of the two, say.\n\nThe purpose of the median is to summarize a set of numbers by a single, typical value. The mean, or average, is also very commonly used for this, but the median is in a sense more typical of the data: it is always one of the data values, unlike the mean, and it is less sensitive to outliers. For instance, the median of a list of a hundred 1's is (rightly) 1, as is the mean. However, if just one of these numbers gets accidentally corrupted to 10,000, the mean shoots up above 100, while the median is unaffected.\n\nComputing the median of n numbers is easy: just sort them. The drawback is that this takes O(n log n) time, whereas we would ideally like something linear. We have reason to be hopeful, because sorting is doing far more work than we really need—we just want the middle element and don't care about the relative ordering of the rest of them.\n\nWhen looking for a recursive solution, it is paradoxically often easier to work with a more general version of the problem—for the simple reason that this gives a more powerful step to recurse upon. In our case, the generalization we will consider is selection.\n\n**SELECTION**  \nInput: A list of numbers S; an integer k  \nOutput: The kth smallest element of S\n\nFor instance, if k = 1, the minimum of S is sought, whereas if k = ⌊|S|/2⌋, it is the median.\n\n## 2.5 Matrix multiplication\n\nThe product of two n × n matrices X and Y is a third n × n matrix Z = XY, with (i, j)th entry\n\nZ_ij = Σ(k=1 to n) X_ik Y_kj.\n\nIn general, XY is not the same as YX; matrix multiplication is not commutative.\n\nThe preceding formula implies an O(n³) algorithm for matrix multiplication: there are n² entries to be computed, and each takes O(n) time. For quite a while, this was widely believed to be the best running time possible, and it was even proved that in certain models of computation no algorithm could do better. It was therefore a source of great excitement when in 1969, the German mathematician Volker Strassen announced a significantly more efficient algorithm, based upon divide-and-conquer.\n\nMatrix multiplication is particularly easy to break into subproblems, because it can be performed blockwise. To see what this means, carve X into four n/2 × n/2 blocks, and also Y:\n\nX = [A B; C D], Y = [E F; G H].\n\nThen their product can be expressed in terms of these blocks and is exactly as if the blocks were single elements:\n\nXY = [AE + BG, AF + BH; CE + DG, CF + DH]\n\nWe now have a divide-and-conquer strategy: to compute the size-n product XY, recursively compute eight size-n/2 products AE, BG, AF, BH, CE, DG, CF, DH, and then do a few O(n²)-time additions. The total running time is described by the recurrence relation\n\nT(n) = 8T(n/2) + O(n²).\n\nThis comes out to an unimpressive O(n³), the same as for the default algorithm. But the efficiency can be further improved, and as with integer multiplication, the key is some clever algebra. It turns out XY can be computed from just seven n/2 × n/2 subproblems, via a decomposition so tricky and intricate that one wonders how Strassen was ever able to discover it! The new running time is\n\nT(n) = 7T(n/2) + O(n²),\n\nwhich by the master theorem works out to O(n^(log₂ 7)) ≈ O(n^2.81).\n\n## 2.6 The fast Fourier transform\n\nWe have so far seen how divide-and-conquer gives fast algorithms for multiplying integers and matrices; our next target is polynomials. The product of two degree-d polynomials is a polynomial of degree 2d, for example:\n\n(1 + 2x + 3x²) · (2 + x + 4x²) = 2 + 5x + 12x² + 11x³ + 12x⁴.\n\nMore generally, if A(x) = a₀ + a₁x + ... + a_dx^d and B(x) = b₀ + b₁x + ... + b_dx^d, their product C(x) = A(x) · B(x) = c₀ + c₁x + ... + c_{2d}x^{2d} has coefficients\n\nc_k = a₀b_k + a₁b_{k-1} + ... + a_kb₀ = Σ(i=0 to k) a_ib_{k-i}\n\n(for i > d, take a_i and b_i to be zero). Computing c_k from this formula takes O(k) steps, and finding all 2d + 1 coefficients would therefore seem to require Θ(d²) time. Can we possibly multiply polynomials faster than this?\n\nThe solution we will develop, the fast Fourier transform, has revolutionized—indeed, defined—the field of signal processing. Because of its huge importance, and its wealth of insights from different fields of study, we will approach it a little more leisurely than usual. The reader who wants just the core algorithm can skip directly to Section 2.6.4."
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 25189,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "output_tokens": 3217,
    "service_tier": "standard"
  }
}