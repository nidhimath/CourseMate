{
  "id": "msg_01DPtevnkvcW9FmdH4nBdje4",
  "type": "message",
  "role": "assistant",
  "model": "claude-opus-4-1-20250805",
  "content": [
    {
      "type": "text",
      "text": "# CS162 Operating Systems and Systems Programming\n## Lecture 12: Scheduling - Linux Schedulers and Fair Sharing\n\n### Goals for Today\nUnderstanding Linux scheduler implementations and proportional fair sharing mechanisms, specifically:\n→ Linux scheduler evolution and design\n→ Proportional fair sharing concepts\n→ Linux EEVDF scheduler\n\n### 12.1 Scheduler Design Goals\n\nWhat we want from schedulers:\n1. Minimize average waiting time for IO/interactive tasks (tasks with short CPU bursts)\n2. Minimize average completion time\n3. Maximize throughput (includes minimizing context switches)\n4. Remain fair/starvation-free\n\n### 12.2 History of Linux Schedulers\n\n**O(n) scheduler** (Linux 2.4 to Linux 2.6)\n→ Scans full list of processes in ready queue at every context switch\n→ Computes priority score for each process\n→ Selects best process to run\n→ Problem: scalability with more threads & multicore CPUs\n\n**O(1) scheduler** (Linux 2.6 to 2.6.22)\n→ Next process chosen in constant time\n→ Priority-based with 140 different priorities\n→ Real-time/kernel tasks: priorities 0-99 (0 is highest)\n→ User tasks: priorities 100-139 (set via \"nice\" command, values -20 to 19)\n\nPer priority-level, each CPU has two ready queues:\n→ Active queue: processes which have not used up time quanta\n→ Expired queue: processes who have\n\nPriority adjustment for user tasks:\n→ Adjusted ±5 based on heuristics\n→ p->sleep_avg = sleep_time - run_time\n→ Higher sleep_avg indicates more I/O bound task\n→ Interactive Credit earned when task sleeps long, spent when runs long\n→ Problem: Very complex to reason about and tune\n\n**Completely Fair Scheduler (CFS)** (Linux 2.6.23 to 6.6)\n\n**EEVDF scheduler** (Linux 6.6 onwards)\n\n### 12.3 Proportional Fair Sharing\n\nDifferent approach from traditional scheduling:\n→ Share CPU proportionally to per-job \"weights\"\n→ Give each job share of CPU according to priority\n→ Low-priority jobs run less often\n→ All jobs make progress (no starvation)\n→ Originated in networking\n→ Maintains global property (fairness) instead of reasoning about queues and priorities\n\n**Fair Sharing Variants:**\n→ Basic: n users each get 1/n of shared resource\n→ Max-min fairness: handles case where user needs less than fair share\n→ Weighted/proportional max-min fairness: assigns weights based on importance\n\n### 12.4 Early Proportional Sharing Approaches\n\n**Lottery Scheduling:**\n→ Give each job lottery tickets (≥1)\n→ On each time slice, randomly pick winning ticket\n→ On average, CPU time proportional to number of tickets\n→ Running time: O(1)\n→ Can suffer temporary unfairness\n→ Cannot lead to starvation or priority inversion\n\n**Stride Scheduling:**\n→ Deterministic proportional fair sharing\n→ Stride of each job = (big number W) / Ni\n→ Larger share of tickets Ni means smaller stride\n→ Each job has pass counter\n→ Scheduler picks job with lowest pass, runs it, adds stride to pass\n→ Low-stride jobs (lots of tickets) run more often\n\nComplications:\n→ What to do when job doesn't use full time slice?\n→ What to do when jobs arrive or leave?\n→ Long delays until job is rescheduled\n\n### 12.5 Fluid Flow Model and GPS\n\n**Fluid Flow Model (Generalized Processor Sharing):**\n→ Assumes infinitely fine-grained context switching\n→ After every instruction (CPU) or bit sent (network)\n→ Proportional fair sharing via weighted bit-by-bit round-robin\n→ During each round, client does work units equal to its weight\n\n**Weighted Fair Queueing (WFQ):**\n→ Select packet that finishes first in GPS assuming no future arrivals\n→ Packet approximation of GPS\n→ Finish time in WFQ at most q + finish time in GPS (q = max time quantum)\n\nImplementation challenge: \n→ Need to compute finish time in fluid flow system\n→ Finish time may change as new packets arrive\n→ Very expensive with 1000s of clients\n\n### 12.6 Virtual Time Solution\n\n**Key observation:** While finish times may change when new packet arrives, order doesn't change!\n\n**Solution:** Track virtual finish time instead of actual finish time\n→ Number of rounds needed to send remaining bits in GPS\n→ Virtual finish time = (length of packet) / wi\n→ System virtual time = index of current round in weighted bit-by-bit round-robin\n\n**System Virtual Time V(t):**\n→ Measures total bit-by-bit round-robin rounds served\n→ V(t) slope = C/N(t)\n→ C: resource capacity\n→ N(t): total weight of backlogged clients at time t\n\n**WFQ Implementation:**\n→ Virtual finish time of packet k+1: Fi^(k+1) = max(V(ai^(k+1)), Fi^k) + Li^(k+1)/wi\n→ Maintain red-black tree of runnable processes sorted by virtual finish time\n→ Always schedule process with earliest virtual finish time\n\n### 12.7 Early Eligible Virtual Deadline First (EEVDF)\n\n**Problem EEVDF solves:** Minimize lag between service received in real system vs fluid flow system\n→ WFQ can have worst case lag O(n)*q\n→ Red client with 1/2 resource capacity can be denied service for n slots\n\n**EEVDF Solution:**\n→ Schedule only eligible processes (virtual arrival time <= current virtual time)\n→ Among eligible, pick earliest deadline\n→ Achieves lag <= O(1)*q (independent of number of clients)\n\n**Configuring \"Packet Lengths\":**\n→ Tasks specify preferred time slice via sched_setattr()\n→ Developers can request interactive tasks be scheduled faster\n→ Cannot \"cheat\" to get more total CPU time with smaller slice\n\n**Implementation in Kernel:**\n→ Track lag in virtual time for each task\n→ Only tasks with lag ≥ 0 are eligible\n→ Compute virtual deadline = eligible_vtime + vtime_slice\n→ Store in sorted red-black tree\n→ Schedule task with lowest deadline\n\n**Handling Sleeping Tasks:**\n→ Remember lag when task sleeps (e.g., on I/O)\n→ Return with that lag when wakes up\n→ Linux decays lag after time to prevent arbitrary delays\n\n**Nice Values to Weights:**\n→ Each priority level is 1.25x weight of next lower\n→ weight = 1024 / 1.25^nice\n\n### 12.8 Comparison Summary\n\nProportional Sharing Algorithm Performance (n = clients, q = max quantum):\n\n**Lottery scheduling:**\n→ Scheduler complexity: O(1)\n→ Finish time delay: O(sqrt(n))*q average\n→ Lag: O(sqrt(n))*q average\n\n**Stride scheduling:**\n→ Scheduler complexity: O(log(n))\n→ Finish time delay: O(log(n))*q\n→ Lag: O(log(n))*q\n\n**WFQ:**\n→ Scheduler complexity: O(log(n))\n→ Finish time delay: q\n→ Lag: O(n)*q\n\n**EEVDF:**\n→ Scheduler complexity: O(log(n))\n→ Finish time delay: q\n→ Lag: O(1)*q\n\n### 12.9 Multicore and Gang Scheduling\n\n**Multicore Scheduling:**\n→ Algorithmically similar to single-core\n→ Per-core scheduling data structures for cache coherence\n→ Affinity scheduling: reschedule thread on same CPU for cache reuse\n\n**Gang Scheduling:**\n→ Schedule cooperating threads together on multi-core\n→ Makes spin-waiting efficient\n→ OS informs parallel programs of processor allocation (Scheduler Activations)\n→ Application adapts to available cores\n→ Space sharing can be efficient due to sublinear parallel speedup\n\n---\n\n# CS162 Operating Systems and Systems Programming\n## Lecture 13: Fair Scheduling Continued & Deadlock\n\n### Goals for Today\n→ Proportional fair sharing and Linux EEVDF (continued)\n→ Deeper understanding of deadlock\n\n### 13.1 Proportional Fair Sharing Review\n\nShare CPU proportionally to per-job \"weights\":\n→ Each job gets CPU share according to priority\n→ Low-priority jobs run less often but make progress\n→ No starvation\n→ Maintains global fairness property\n\nFair sharing variants handle different scenarios:\n→ Basic: equal sharing (1/n each)\n→ Max-min: when users need less than fair share\n→ Weighted: importance-based allocation\n\n### 13.2 Fair Sharing Algorithms Review\n\n**Lottery Scheduling:**\n→ Lottery tickets assigned to jobs\n→ Random winner selection each time slice\n→ O(1) runtime\n→ Average CPU time proportional to tickets\n\n**Stride Scheduling:**\n→ Deterministic with stride = W/Ni\n→ Pass counter advanced by stride\n→ Challenges with sleeping/joining/leaving jobs\n\n### 13.3 Virtual Time and WFQ\n\n**Fluid Flow Model (GPS):**\n→ Infinitely fine-grained context switching\n→ Weighted bit-by-bit round-robin\n\n**Virtual Time Solution:**\n→ Track rounds needed instead of finish times\n→ Order doesn't change when packets arrive\n→ System virtual time V(t) increases inversely to total weight\n\n**WFQ Implementation:**\n→ Compute virtual finish time for new work\n→ Red-black tree sorted by virtual finish time\n→ Schedule earliest virtual finish time\n\n### 13.4 EEVDF Algorithm\n\n**Problem:** WFQ can have O(n) lag vs fluid flow system\n→ High-weight clients denied service for n slots\n\n**EEVDF Solution:**\n→ Schedule only eligible processes (virtual arrival <= current virtual time)\n→ Pick earliest deadline among eligible\n→ Achieves O(1) lag independent of client count\n\n**Implementation Details:**\n→ Track lag in virtual time\n→ Compute virtual deadline for eligible tasks (lag ≥ 0)\n→ Red-black tree maintains sorted order\n→ Schedule lowest deadline\n\n**Packet Length Configuration:**\n→ Tasks specify preferred slice via sched_setattr()\n→ Cannot cheat for more total CPU time\n→ Enables faster scheduling of interactive tasks\n\n### 13.5 Understanding Deadlock\n\n**Deadlock:** Cyclic waiting for resources\n→ Thread A owns Res 1, waits for Res 2\n→ Thread B owns Res 2, waits for Res 1\n→ System stuck in circular dependency\n\n**Deadlock vs Starvation:**\n→ Deadlock implies starvation\n→ Starvation doesn't imply deadlock\n→ Starvation can end (but doesn't have to)\n→ Deadlock cannot end without external intervention\n\n### 13.6 Deadlock Examples\n\n**Bridge Crossing Example:**\n→ Single-lane bridge, traffic from both directions\n→ Cars must own segment underneath\n→ Must acquire segment moving into\n→ Bridge allows traffic one direction at time\n→ Circular waiting creates deadlock\n\n**Lock-based Deadlock:**\nThread A: x.Acquire(); y.Acquire(); ... y.Release(); x.Release();\nThread B: y.Acquire(); x.Acquire(); ... x.Release(); y.Release();\n→ Non-deterministic deadlock possible\n→ Sometimes schedule avoids deadlock\n\n**Dining Computer Scientists:**\n→ Five chopsticks, five scientists\n→ Need two chopsticks to eat\n→ Free-for-all leads to deadlock\n→ External intervention needed\n\n### 13.7 Four Requirements for Deadlock\n\n1. **Mutual exclusion and bounded resources:**\n   Only one thread can use resource at time\n\n2. **Hold and wait:**\n   Thread holding resource waits for additional resources\n\n3. **No preemption:**\n   Resources released only voluntarily after use\n\n4. **Circular wait:**\n   Set {T1, ..., Tn} where Ti waits for resource held by T(i+1)\n\n### 13.8 Detecting Deadlock\n\n**Resource-Allocation Graph:**\n→ Vertices: threads T and resources R\n→ Request edge: Ti → Rj\n→ Assignment edge: Rj → Ti\n→ Cycle may indicate deadlock\n\n**Detection Algorithm:**\n→ Track FreeResources, RequestT, AllocT vectors\n→ Simulate if threads can terminate\n→ Start with Avail = FreeResources\n→ Find threads whose requests <= Avail\n→ Release their resources, update Avail\n→ Threads remaining in UNFINISHED are deadlocked\n\n### 13.9 Dealing with Deadlock\n\n**Four approaches:**\n\n1. **Deadlock prevention:** Write code to avoid deadlock conditions\n2. **Deadlock recovery:** Let deadlock happen, then recover\n3. **Deadlock avoidance:** Dynamically delay requests to prevent deadlock\n4. **Deadlock denial:** Ignore possibility of deadlock\n\n### 13.10 Deadlock Prevention Techniques\n\n**Fix Condition 1 (Mutual Exclusion):**\n→ Provide sufficient resources\n→ Example: virtual memory provides \"infinite\" space\n\n**Fix Condition 2 (Hold and Wait):**\n→ Acquire all resources atomically\n→ Acquire_both(x, y) instead of separate acquisitions\n\n**Fix Condition 3 (No Preemption):**\n→ Force thread to give up resource\n→ Common in databases (transaction aborts)\n→ Wireless networks: collision detection and retry\n\n**Fix Condition 4 (Circular Wait):**\n→ Force all threads to request resources in same order\n→ Example: always acquire lower-numbered resource first\n\n### 13.11 Deadlock Avoidance\n\n**Three States:**\n→ Safe state: Can delay acquisition to prevent deadlock\n→ Unsafe state: No deadlock yet, but unavoidable\n→ Deadlocked state: Deadlock exists\n\n**Banker's Algorithm:**\n→ Ensures system never enters unsafe state\n→ Evaluate each request, grant if deadlock-free ordering exists\n→ Pretend request granted, run detection algorithm\n→ Uses Max allocation instead of current Request\n→ If safe after grant, allow; if unsafe, delay\n\nAlgorithm modification for avoidance:\n→ Check if ([Maxthread] - [Allocthread] <= [Avail])\n→ Instead of ([Requestthread] <= [Avail])\n→ Ensures threads can complete with maximum possible requests\n\n### 13.12 Summary\n\n**Deadlock Key Points:**\n→ Deadlock implies starvation, not vice versa\n→ Four necessary conditions must all be present\n→ Prevention eliminates one condition\n→ Recovery handles after occurrence\n→ Avoidance dynamically prevents unsafe states\n→ Banker's algorithm maintains system safety\n\n**Scheduler Evolution:**\n→ O(n) → O(1) → CFS → EEVDF\n→ From simple scanning to complex heuristics\n→ Now proportional fair sharing with low lag\n→ EEVDF provides predictable, fair scheduling with minimal complexity"
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 15680,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "output_tokens": 3713,
    "service_tier": "standard"
  }
}